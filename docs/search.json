[{"path":"index.html","id":"машинне-навчання","chapter":"Машинне навчання","heading":"Машинне навчання","text":"Ігор Мірошниченко2022-05-06","code":""},{"path":"index.html","id":"передмова","chapter":"Машинне навчання","heading":"Передмова","text":"Наразі підручник в процесі розробки.Якщо ви помітили неточності або помилки, будь-ласка напишіть мені: ihor.miroshnychenko@kneu.ua","code":""},{"path":"tidymodels.html","id":"tidymodels","chapter":" 1 Tidymodels","heading":" 1 Tidymodels","text":"Tidymodels - фреймворк для побудови моделей машинного навчання за допомогою мови програмування R.Розглянемо датасет з пінгвінами, який прийшов на заміну класичному датасету про іриси.Він добре підходить як під задачі регресії, так і під задачі класифікації.","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\npenguins\n## # A tibble: 344 x 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # ... with 334 more rows, and 2 more variables: sex <fct>, year <int>\n\nglimpse(penguins)\n## Rows: 344\n## Columns: 8\n## $ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ade~\n## $ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgers~\n## $ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1,~\n## $ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1,~\n## $ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 18~\n## $ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475,~\n## $ sex               <fct> male, female, female, NA, female, male, female, mal~\n## $ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200~\n\npenguins <- penguins %>% \n  relocate(body_mass_g)"},{"path":"tidymodels.html","id":"розбиття-вибірки-на-тестову-та-навчальну.","chapter":" 1 Tidymodels","heading":"1.1 Розбиття вибірки на тестову та навчальну.","text":"Звичайне розбиття","code":"\nset.seed(2022)\npenguins_split <- initial_split(penguins, prop = .8)\n\npenguins_split\n## <Analysis/Assess/Total>\n## <275/69/344>\n\npenguins_train <- training(penguins_split)\n\npenguins_test  <-  testing(penguins_split)\n\ndim(penguins_train)\n## [1] 275   8\nset.seed(2022)\n\npenguins_split <- initial_split(penguins, prop = .8, strata = sex)\n\npenguins_split\n## <Analysis/Assess/Total>\n## <275/69/344>\n\npenguins_train <- training(penguins_split)\n\npenguins_test  <-  testing(penguins_split)\n\ndim(penguins_train)\n## [1] 275   8\n\npenguins_train %>% \n  count(sex)\n## # A tibble: 3 x 2\n##   sex        n\n##   <fct>  <int>\n## 1 female   131\n## 2 male     135\n## 3 <NA>       9\n\npenguins_test %>% \n  count(sex)\n## # A tibble: 3 x 2\n##   sex        n\n##   <fct>  <int>\n## 1 female    34\n## 2 male      33\n## 3 <NA>       2"},{"path":"tidymodels.html","id":"побудова-моделей-за-допомогою-tidymodels","chapter":" 1 Tidymodels","heading":"1.2 Побудова моделей за допомогою Tidymodels","text":"","code":"\nlinear_reg() %>% \n  set_engine(\"lm\") %>% \n  translate()\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm \n## \n## Model fit template:\n## stats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nlinear_reg(penalty = 1) %>% \n  set_engine(\"glmnet\") %>% \n  translate()\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = 1\n## \n## Computational engine: glmnet \n## \n## Model fit template:\n## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n##     family = \"gaussian\")\n\nlinear_reg() %>% \n  set_engine(\"stan\") %>% \n  translate()\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: stan \n## \n## Model fit template:\n## rstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n##     weights = missing_arg(), family = stats::gaussian, refresh = 0)\nlm_model <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\nlm_form_fit <- \n  lm_model %>% \n  fit(body_mass_g ~ bill_length_mm + bill_depth_mm, data = penguins_train)\n\nlm_xy_fit <- \n  lm_model %>% \n  fit_xy(\n    x = penguins_train %>% select(bill_length_mm, bill_depth_mm),\n    y = penguins_train %>% pull(body_mass_g)\n  )\n\nlm_form_fit\n## parsnip model object\n## \n## \n## Call:\n## stats::lm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm, \n##     data = data)\n## \n## Coefficients:\n##    (Intercept)  bill_length_mm   bill_depth_mm  \n##        3698.15           69.79         -150.48\n\nlm_xy_fit\n## parsnip model object\n## \n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##    (Intercept)  bill_length_mm   bill_depth_mm  \n##        3698.15           69.79         -150.48\nlm_form_fit %>% \n  extract_fit_engine()\n## \n## Call:\n## stats::lm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm, \n##     data = data)\n## \n## Coefficients:\n##    (Intercept)  bill_length_mm   bill_depth_mm  \n##        3698.15           69.79         -150.48\n\nlm_form_fit %>% \n  extract_fit_engine() %>% \n  vcov()\n##                (Intercept) bill_length_mm bill_depth_mm\n## (Intercept)     240286.265    -2494.52933   -7522.27997\n## bill_length_mm   -2494.529       45.19213      29.53007\n## bill_depth_mm    -7522.280       29.53007     362.09611\n\nmodel_res <- \n  lm_form_fit %>% \n  extract_fit_engine() %>% \n  summary()\n\nparam_est <- coef(model_res)\n\nclass(param_est)\n## [1] \"matrix\" \"array\"\n\nparam_est\n##                  Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)    3698.15105 490.190030  7.544321 6.970546e-13\n## bill_length_mm   69.79313   6.722509 10.382006 1.820525e-21\n## bill_depth_mm  -150.47740  19.028823 -7.907867 6.733650e-14\n\nlm_form_fit %>% \n  tidy()\n## # A tibble: 3 x 5\n##   term           estimate std.error statistic  p.value\n##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)      3698.     490.        7.54 6.97e-13\n## 2 bill_length_mm     69.8      6.72     10.4  1.82e-21\n## 3 bill_depth_mm    -150.      19.0      -7.91 6.73e-14\n\nlm_form_fit %>% \n  glance()\n## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.449         0.445  599.      110. 1.16e-35     2 -2132. 4271. 4286.\n## # ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\npenguins_test_small <- penguins_test %>%\n  slice(1:5)\n\npredict(lm_form_fit, new_data = penguins_test_small)\n## # A tibble: 5 x 1\n##   .pred\n##   <dbl>\n## 1 3590.\n## 2 3918.\n## 3 3574.\n## 4 3540.\n## 5 3673.\n\npenguins_test_small %>% \n  select(body_mass_g) %>% \n  bind_cols(predict(lm_form_fit, penguins_test_small)) %>% \n  bind_cols(predict(lm_form_fit, penguins_test_small, type = \"pred_int\")) \n## # A tibble: 5 x 4\n##   body_mass_g .pred .pred_lower .pred_upper\n##         <int> <dbl>       <dbl>       <dbl>\n## 1        4250 3590.       2404.       4775.\n## 2        3200 3918.       2737.       5099.\n## 3        3700 3574.       2390.       4758.\n## 4        3450 3540.       2356.       4724.\n## 5        4200 3673.       2481.       4866."},{"path":"tidymodels.html","id":"створення-робочого-процесу-для-побудови-моделей","chapter":" 1 Tidymodels","heading":"1.3 Створення робочого процесу для побудови моделей","text":"","code":"\nlm_model <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model)\n\nlm_wflow\n## == Workflow ===================================================================\n## Preprocessor: None\n## Model: linear_reg()\n## \n## -- Model ----------------------------------------------------------------------\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nlm_wflow <- \n  lm_wflow %>% \n  add_formula(body_mass_g ~ bill_length_mm + bill_depth_mm)\n\nlm_fit <- fit(lm_wflow, penguins_train)\n\nlm_fit\n## == Workflow [trained] =========================================================\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## body_mass_g ~ bill_length_mm + bill_depth_mm\n## \n## -- Model ----------------------------------------------------------------------\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##    (Intercept)  bill_length_mm   bill_depth_mm  \n##        3698.15           69.79         -150.48\n\npredict(lm_fit, penguins_train %>% \n          slice(1:3))\n## # A tibble: 3 x 1\n##   .pred\n##   <dbl>\n## 1 3837.\n## 2 3802.\n## 3 3355.\n\nlm_fit %>% \n  update_formula(body_mass_g ~ bill_length_mm)\n## == Workflow ===================================================================\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## body_mass_g ~ bill_length_mm\n## \n## -- Model ----------------------------------------------------------------------\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\nlm_wflow <- \n  lm_wflow %>% \n  remove_formula() %>% \n  add_variables(outcome = body_mass_g, predictors = c(contains(\"_\")))\n\nlm_wflow\n## == Workflow ===================================================================\n## Preprocessor: Variables\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## Outcomes: body_mass_g\n## Predictors: c(contains(\"_\"))\n## \n## -- Model ----------------------------------------------------------------------\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nfit(lm_wflow, penguins_train)\n## == Workflow [trained] =========================================================\n## Preprocessor: Variables\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## Outcomes: body_mass_g\n## Predictors: c(contains(\"_\"))\n## \n## -- Model ----------------------------------------------------------------------\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##       (Intercept)     bill_length_mm      bill_depth_mm  flipper_length_mm  \n##         -6656.034              2.846             21.343             51.511\nformulas <- list(\n  bill_length = body_mass_g ~ bill_length_mm,\n  bill_depth = body_mass_g ~ bill_depth_mm,\n  bill_length_depth = body_mass_g ~ bill_length_mm + bill_depth_mm + sex,\n  flipper = body_mass_g ~ flipper_length_mm\n)\n\nlibrary(workflowsets)\n\nformulas_model <- workflow_set(preproc = formulas, models = list(lm = lm_model))\n\nformulas_model\n## # A workflow set/tibble: 4 x 4\n##   wflow_id             info             option    result    \n##   <chr>                <list>           <list>    <list>    \n## 1 bill_length_lm       <tibble [1 x 4]> <opts[0]> <list [0]>\n## 2 bill_depth_lm        <tibble [1 x 4]> <opts[0]> <list [0]>\n## 3 bill_length_depth_lm <tibble [1 x 4]> <opts[0]> <list [0]>\n## 4 flipper_lm           <tibble [1 x 4]> <opts[0]> <list [0]>\n\nformulas_model$info[[1]]\n## # A tibble: 1 x 4\n##   workflow   preproc model      comment\n##   <list>     <chr>   <chr>      <chr>  \n## 1 <workflow> formula linear_reg \"\"\n\nextract_workflow(formulas_model, id = \"flipper_lm\")\n## == Workflow ===================================================================\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## body_mass_g ~ flipper_length_mm\n## \n## -- Model ----------------------------------------------------------------------\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nformulas_model <-\n   formulas_model %>%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], penguins_train)))\n\nformulas_model\n## # A workflow set/tibble: 4 x 5\n##   wflow_id             info             option    result     fit       \n##   <chr>                <list>           <list>    <list>     <list>    \n## 1 bill_length_lm       <tibble [1 x 4]> <opts[0]> <list [0]> <workflow>\n## 2 bill_depth_lm        <tibble [1 x 4]> <opts[0]> <list [0]> <workflow>\n## 3 bill_length_depth_lm <tibble [1 x 4]> <opts[0]> <list [0]> <workflow>\n## 4 flipper_lm           <tibble [1 x 4]> <opts[0]> <list [0]> <workflow>\n\nformulas_model$fit[[1]]\n## == Workflow [trained] =========================================================\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## body_mass_g ~ bill_length_mm\n## \n## -- Model ----------------------------------------------------------------------\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n##    (Intercept)  bill_length_mm  \n##         572.09           82.07"},{"path":"tidymodels.html","id":"фіча-інженірінг","chapter":" 1 Tidymodels","heading":"1.4 Фіча інженірінг","text":"","code":"\n\npenguins_train\n## # A tibble: 275 x 8\n##    body_mass_g species island    bill_length_mm bill_depth_mm flipper_length_mm\n##          <int> <fct>   <fct>              <dbl>         <dbl>             <int>\n##  1        3800 Adelie  Torgersen           39.5          17.4               186\n##  2        3250 Adelie  Torgersen           40.3          18                 195\n##  3        3450 Adelie  Torgersen           36.7          19.3               193\n##  4        3625 Adelie  Torgersen           38.9          17.8               181\n##  5        3475 Adelie  Torgersen           34.1          18.1               193\n##  6        3300 Adelie  Torgersen           37.8          17.1               186\n##  7        3700 Adelie  Torgersen           37.8          17.3               180\n##  8        3325 Adelie  Torgersen           34.4          18.4               184\n##  9        3400 Adelie  Biscoe              37.8          18.3               174\n## 10        3800 Adelie  Biscoe              35.9          19.2               189\n## # ... with 265 more rows, and 2 more variables: sex <fct>, year <int>\n\nsimple_penguins <- \n  recipe(body_mass_g ~ bill_length_mm + bill_depth_mm + sex,\n         data = penguins_train) %>%\n  step_log(bill_depth_mm, base = 10) %>% \n  step_dummy(all_nominal_predictors())\n\nsimple_penguins\n## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          3\n## \n## Operations:\n## \n## Log transformation on bill_depth_mm\n## Dummy variables from all_nominal_predictors()\n# lm_wflow %>% \n# add_recipe(simple_penguins)\n\nlm_wflow <- \n  lm_wflow %>% \n  remove_variables() %>% \n  add_recipe(simple_penguins)\n\nlm_wflow\n## == Workflow ===================================================================\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## -- Preprocessor ---------------------------------------------------------------\n## 2 Recipe Steps\n## \n## * step_log()\n## * step_dummy()\n## \n## -- Model ----------------------------------------------------------------------\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nlm_fit <- fit(lm_wflow, penguins_train)\n\npredict(lm_fit, penguins_train %>% slice(1:3))\n## # A tibble: 3 x 1\n##   .pred\n##   <dbl>\n## 1 3479.\n## 2 3351.\n## 3 2923.\n\nlm_fit %>% \n  tidy()\n## # A tibble: 4 x 5\n##   term           estimate std.error statistic  p.value\n##   <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     15141.     897.       16.9  9.06e-44\n## 2 bill_length_mm     31.3      5.73      5.46 1.10e- 7\n## 3 bill_depth_mm  -10396.     636.      -16.3  7.54e-42\n## 4 sex_male          958.      66.3      14.5  3.16e-35\npenguins_train %>% \n  ggplot(aes(sex)) +\n  geom_bar()\n\nsimple_penguins <- \n  recipe(body_mass_g ~ bill_length_mm + bill_depth_mm + sex,\n         data = penguins_train) %>%\n  step_log(bill_depth_mm, base = 10) %>% \n  step_dummy(all_nominal_predictors()) %>% # one_hot = TRUE\n  step_unknown(sex, new_level = \"unknown sex\")\n\n# step_novel()\n# step_other(var_name, threshold = 0.01)\n\nsimple_penguins\n## Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          3\n## \n## Operations:\n## \n## Log transformation on bill_depth_mm\n## Dummy variables from all_nominal_predictors()\n## Unknown factor level assignment for sex\nggplot(penguins_train, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point() + \n  facet_wrap(~ species) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"lightblue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Довжина плавника\", y = \"Вага\")\n\n# body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex + flipper_length_mm:sex\n\n# body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm*sex\n\nsimple_penguins <- \n  recipe(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex,\n         data = penguins_train) %>%\n  step_log(bill_depth_mm, base = 10) %>% \n  step_dummy(all_nominal_predictors()) %>% # one_hot = TRUE\n  step_unknown(sex, new_level = \"unknown sex\") %>% \n  step_interact( ~ flipper_length_mm:sex)\nlibrary(patchwork)\nlibrary(splines)\n\nplot_smoother <- function(deg_free) {\n  ggplot(penguins_train, aes(x = bill_depth_mm, y = body_mass_g)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Spline Terms\"),\n         y = \"Вага\")\n}\n\n( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(30) )\n\nsimple_penguins <- \n  recipe(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex,\n         data = penguins_train) %>%\n  step_log(bill_depth_mm, base = 10) %>% \n  step_dummy(all_nominal_predictors()) %>% # one_hot = TRUE\n  step_unknown(sex, new_level = \"unknown sex\") %>% \n  step_interact( ~ flipper_length_mm:sex) %>% \n  step_ns(bill_depth_mm, deg_free = 5)\ntidy(simple_penguins)\n## # A tibble: 5 x 6\n##   number operation type     trained skip  id            \n##    <int> <chr>     <chr>    <lgl>   <lgl> <chr>         \n## 1      1 step      log      FALSE   FALSE log_drhW1     \n## 2      2 step      dummy    FALSE   FALSE dummy_rDO3E   \n## 3      3 step      unknown  FALSE   FALSE unknown_Itj5p \n## 4      4 step      interact FALSE   FALSE interact_RfO0k\n## 5      5 step      ns       FALSE   FALSE ns_6YYfP\n\nsimple_penguins <- \n  recipe(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + sex,\n         data = penguins_train) %>%\n  step_log(bill_depth_mm, base = 10) %>% \n  step_unknown(sex, new_level = \"unknown sex\", id = \"my_id\") # add id\n#  step_dummy(all_nominal_predictors()) %>% # one_hot = TRUE\n#  step_interact( ~ flipper_length_mm:sex) %>% \n#  step_ns(bill_depth_mm, deg_free = 2)\n\ntidy(simple_penguins)\n## # A tibble: 2 x 6\n##   number operation type    trained skip  id       \n##    <int> <chr>     <chr>   <lgl>   <lgl> <chr>    \n## 1      1 step      log     FALSE   FALSE log_M3wCq\n## 2      2 step      unknown FALSE   FALSE my_id\n\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_recipe(simple_penguins)\n\nlm_fit <- fit(lm_wflow, penguins_train)\n\nestimated_recipe <- lm_fit %>% \n  extract_recipe(estimated = TRUE)\npenguins_test_res <- predict(lm_fit, new_data = penguins_train %>%\n                               select(-body_mass_g))\n\npenguins_test_res\n## # A tibble: 275 x 1\n##    .pred\n##    <dbl>\n##  1 3327.\n##  2 3617.\n##  3 3441.\n##  4 3103.\n##  5 3549.\n##  6 3363.\n##  7 3117.\n##  8 3174.\n##  9 2797.\n## 10 3298.\n## # ... with 265 more rows\n\n\npenguins_test_res <- bind_cols(penguins_test_res, penguins_train %>%\n                                 select(body_mass_g))\n\npenguins_test_res\n## # A tibble: 275 x 2\n##    .pred body_mass_g\n##    <dbl>       <int>\n##  1 3327.        3800\n##  2 3617.        3250\n##  3 3441.        3450\n##  4 3103.        3625\n##  5 3549.        3475\n##  6 3363.        3300\n##  7 3117.        3700\n##  8 3174.        3325\n##  9 2797.        3400\n## 10 3298.        3800\n## # ... with 265 more rows\n\nggplot(penguins_test_res, aes(x = body_mass_g, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Body Mass\", x = \"Body Mass\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\nrmse(penguins_test_res, truth = body_mass_g, estimate = .pred)\n## # A tibble: 1 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        345.\n\npenguins_metrics <- metric_set(rmse, rsq, mae)\npenguins_metrics(penguins_test_res, truth = body_mass_g, estimate = .pred)\n## # A tibble: 3 x 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard     345.   \n## 2 rsq     standard       0.815\n## 3 mae     standard     284."},{"path":"reg.html","id":"reg","chapter":" 2 Лінійна регресія","heading":" 2 Лінійна регресія","text":"Модель лінійної регресії:\n\\[\\hat{y} = \\beta_0 + \\sum \\beta_j x_j\\]\n\\(\\beta_0\\) - вільний коефіцієнт (bias, intercept)\\(\\beta_j\\) - ваговий коефіцієнт (weights)\\(\\beta_0, \\beta_1, \\dots, \\beta_n\\) - параметриІнколи можуть скорочувати запис формули:\n\\[\\hat{y} = \\beta_0 + \\left \\langle \\beta, x \\right \\rangle\\]$$$$","code":""},{"path":"reg.html","id":"коли-такі-моделі-можна-використовувати","chapter":" 2 Лінійна регресія","heading":"2.1 Коли такі моделі можна використовувати?","text":"","code":""},{"path":"reg.html","id":"номінативні-показники-one-hot-encoding","chapter":" 2 Лінійна регресія","heading":"2.1.1 Номінативні показники: one-hot encoding","text":"Припустимо, що один з наших показників \\(x_j\\) номінативний і він приймає значення з певної множини значень \\(x_j \\\\left \\{ c_1, c_2, \\dots, c_m \\right \\}\\)Основна мета one-hot encoding створити нові бінарні змінні з якими модель регресії зможе працювати:\\(b_i(x) = \\left [ f_i(x) = c_i \\right ]\\)Нотація Айверсона:\n\\[\\left [ \\models  \\right ] = 1\\]\n\\[\\left [ ⊭  \\right ] = 0\\]\\(b_1(x), b_2(x), \\dots, b_m(x)\\) - нові показники.Як в такому випадку буде виглядати модель?\\[\\hat{y} = \\beta_0 + \\beta_1\\left [ b(x) = c_1 \\right ] + \\dots + \\beta_m\\left [ b(x) = c_m \\right ] + \\dots\\]","code":""},{"path":"reg.html","id":"бінаризація","chapter":" 2 Лінійна регресія","heading":"2.1.2 Бінаризація","text":"В випадках, коли в нас існує нелінійна залежність між \\(y\\) та \\(x\\) є сенс використати бінеризацію.\nБудуємо певну сітку значень \\({t_1, t_2, \\dots, t_m}\\), тоді нові показники задамо як:\\[b_i(x) = [t_{-1} < x_j \\leqslant t_{}], \\;\\; = 1,\\dots,m+1\\]\nЛінійна модель набуває вигляду:\n\\[\\hat{y} = \\beta_1\\left [ t_{-1} < x_j \\leqslant t_{} \\right ] + \\dots + \\beta_m\\left [ t_{m} < x_j \\leqslant t_{m+1} \\right ] + \\dots\\]\nМежі інтервалів можна подавати, як перцентилі.","code":""},{"path":"reg.html","id":"текстові-дані-bag-of-words","chapter":" 2 Лінійна регресія","heading":"2.1.3 Текстові дані: bag of words","text":"","code":""},{"path":"reg.html","id":"похибки-в-задачах-регресії","chapter":" 2 Лінійна регресія","heading":"2.2 Похибки в задачах регресії","text":"\\[L(y, \\hat{y})\\]\n### Квадратична функція похибок\n\\[L(y, \\hat{y}) = (y - \\hat{y})^2\\]\\[MSE(\\hat{y}, x) = \\frac{1}{l}\\sum(y - \\hat{y}(x))^2\\]\\[RMSE(\\hat{y}, x) = \\sqrt{\\frac{1}{l}\\sum(y - \\hat{y}(x))^2}\\]\n\\[R^2(\\hat{y}, x) = 1 - \\frac{\\sum(y - \\hat{y})^2}{\\sum(y - \\overline{y})^2}\\]","code":""},{"path":"reg.html","id":"абсолютна-функція-похибок","chapter":" 2 Лінійна регресія","heading":"2.2.1 Абсолютна функція похибок","text":"\\[L(y, \\hat{y}) = |y - \\hat{y}|\\]\\[MAE(\\hat{y}, x) = \\frac{1}{l}\\sum|y - \\hat{y}|\\]\nПозитивні сторони: стійка до викидівНегативні сторони: похідна не має інформації про близькість екстремуму + не має похідної в нулі.","code":""},{"path":"reg.html","id":"huber-loss","chapter":" 2 Лінійна регресія","heading":"2.2.2 Huber loss","text":"Поєднання квадратичної та абсолютної функції. Необхідно підбирати дельту. Не має другої похідної.\n\\[\nL_{\\delta}(y, \\hat{y})\\left\\{\\begin{matrix}\n\\frac{1}{2}(y - \\hat{y})^2, & |y - \\hat{y}| <  \\delta \\\\ \n\\delta (|y - \\hat{y}| - \\frac{1}{2}\\delta) & |y - \\hat{y}| \\geqslant   \\delta   \n\\end{matrix}\\right.\n\\]","code":""},{"path":"reg.html","id":"log-cosh","chapter":" 2 Лінійна регресія","heading":"2.2.3 Log-Cosh","text":"Використовується гіперболічний косинус\n\\[\nL_{\\delta}(y, \\hat{y}) = log (cosh(y - \\hat{y}))\n\\]","code":""},{"path":"reg.html","id":"msle","chapter":" 2 Лінійна регресія","heading":"2.2.4 MSLE","text":"Mean squared logarithmic error\n\\[\nL(y, \\hat{y}) = (log(\\hat{y} + 1) - log(y + 1))^2\n\\]","code":""},{"path":"reg.html","id":"відносні-функції-помилок","chapter":" 2 Лінійна регресія","heading":"2.2.5 Відносні функції помилок","text":"","code":""},{"path":"reg.html","id":"mape","chapter":" 2 Лінійна регресія","heading":"2.2.5.1 MAPE","text":"\\[\nL_{\\delta}(y, \\hat{y}) = |\\frac{y - \\hat{y}}{y}|\n\\]","code":""},{"path":"reg.html","id":"smape","chapter":" 2 Лінійна регресія","heading":"2.2.5.2 SMAPE","text":"\\[\nL_{\\delta}(y, \\hat{y}) = \\frac{|y - \\hat{y}|}{(|y| + |\\hat{y}|)/2}\n\\]","code":""},{"path":"reg.html","id":"квантильна-функції","chapter":" 2 Лінійна регресія","heading":"2.2.6 Квантильна функції","text":"Можна регулювати штраф за завищення і заниження похибокФункцію помилок потрібно підбирати в залежності від задачі.","code":""},{"path":"reg.html","id":"перенавчання","chapter":" 2 Лінійна регресія","heading":"2.3 Перенавчання","text":"Нерідко в моделі машинного навчання стикаються з ситуацією перенавчання — якість моделі на нових даних значно гірша ніж на навчальній вибірці. Тому важливо щоб наша модель вміла узагалювати свої результати на нові дані.Для візуалізації цього ефекту проведемо симуляцію:Візуалізація демострує, що прості моделі мають недостатню точність, а складні моделі занадто добре підлаштовуються під вибірку, через що стають непридатними для подальшого використання.Існує декілька варіантів виходу з ситуації перенавчання:регуляризація: штрафування моделі за складністьрегуляризація: штрафування моделі за складністькрос-валідація: побудова низки моделей на підвибірках данихкрос-валідація: побудова низки моделей на підвибірках данихзбільшення розмірності вибірки (про цей варіант часто забувають).збільшення розмірності вибірки (про цей варіант часто забувають).","code":"\nlibrary(tidyverse)\nlibrary(patchwork)\n\nset.seed(1234)\n\ndf <- tibble(x = seq(1, 2, 0.05),\n             y = cos(1.5 * pi * x) + rnorm(x, 0, 0.1))\n\nplots <- map(c(1, 4, 15), function(d){\n  ggplot(df, aes(x, y)) +\n    geom_point() + \n    # geom_smooth(se = FALSE, color = \"#2F6B57\") + \n    geom_smooth(method = \"lm\", formula = y ~ poly(x, d), se = FALSE, color = \"#2F6B57\") +\n    ggtitle(paste(\"Poly \", d))\n})\n\nplots[[1]] / plots[[2]] / plots[[3]]"},{"path":"reg.html","id":"оцінювання-якості-моделей","chapter":" 2 Лінійна регресія","heading":"2.4 Оцінювання якості моделей","text":"Адекватна оцінка якості моделі грунтується на підході відкладеної вибірки: розмічені дані (дані, з відомими відповідями) розбиваються на дві частини: навчальну та тестову. На навчальній вибірці модель навчається а на тестовій перевіряється її якість. Якщо показник якості моделі на тестовій вибірці задовольняє наші потреби, можемо вважати, що модель знайшла певні закономірності в даних.Але бувають випадки, коли якість моделі залежить від “формату” розбиття на підвибірки: ми можемо отримати різну якість моделі, якщо розіб’ємо дані за інших пропорцій або іншого початкового значення генератора випадкових величин. Вирішити таку проблему можна за допомогою крос-валідації. Дані розбиваються на \\(k\\) блоків \\(X_1, X_2,\\dots,X_k\\) приблизно однакового розміру. Після чого будується \\(k\\) моделей \\(\\hat{y_1},\\dots,\\hat{y_k}\\), при чому кожна \\(k\\) модель навчається на всіх блоках окрім \\(k\\). Після чого кожна модель оцінюється оцінюється по блоку який не приймав в навчанні, а результати усереднюються:\\[CV = \\frac{1}{k}\\sum{Q(\\hat{y_i}, X_i)}\\]Як отримати фінальну модель для подальшого використання? Два варіанта:Навчаємо модель на всій вибірці даних, її параметри будуть підібрані на більшій кількості спостережень і можемо сподіватися, що якість моделі зросте.Навчаємо модель на всій вибірці даних, її параметри будуть підібрані на більшій кількості спостережень і можемо сподіватися, що якість моделі зросте.Будуємо композицію моделі з \\(\\hat{y_1},\\dots,\\hat{y_k}\\): наприклад, усереднення прогнозів всіх моделей, що може привести до підвищення стійкості моделі.Будуємо композицію моделі з \\(\\hat{y_1},\\dots,\\hat{y_k}\\): наприклад, усереднення прогнозів всіх моделей, що може привести до підвищення стійкості моделі.","code":""},{"path":"reg.html","id":"градієнтний-спуск","chapter":" 2 Лінійна регресія","heading":"2.5 Градієнтний спуск","text":"Пізніше…","code":""},{"path":"reg.html","id":"приклад-використання","chapter":" 2 Лінійна регресія","heading":"2.6 Приклад використання","text":"bookdown::render_book(“index.Rmd,” output_dir = “docs”)","code":"\nfm0 <- lm(dist ~ speed, cars)\nX <- model.matrix(fm0)\nf <- function(b) with(cars, sum(log(cosh(dist - X %*% b))))\nres <- optim(coef(fm0), f, method = \"BFGS\")\nres$par\n## (Intercept)       speed \n##  -12.816190    3.469536\n\nf <- function(b) with(cars, mean(abs((dist - X %*% b))))\nres <- optim(c(100, 100), f, method = \"L-BFGS-B\")\nres$par\n## [1] -11.687387   3.403979"},{"path":"classification.html","id":"classification","chapter":" 3 Класифікація","heading":" 3 Класифікація","text":"","code":""},{"path":"references.html","id":"references","chapter":"Література","heading":"Література","text":"text","code":""}]
